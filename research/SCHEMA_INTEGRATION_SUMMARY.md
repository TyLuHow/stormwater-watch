# Data Schema Integration - Executive Summary

**Research Date:** December 6, 2025
**Completion Status:** ‚úÖ Complete
**Full Report:** `./data-schema-integration-analysis.md`
**Proposed Schema:** `../prisma/schema-integrated-proposed.prisma`

---

## What Was Accomplished

### 1. Data Collection ‚úÖ
- Downloaded 5,000 sample records from each of 7 SMARTS datasets (~35,000 records total)
- Analyzed ~14 MB of real production data
- Documented current eSMR schema structure
- Total datasets analyzed: 8 (1 eSMR + 7 SMARTS)

### 2. Schema Analysis ‚úÖ
- Mapped all 32 fields in Violations dataset
- Mapped all 41 fields in Enforcement Actions dataset
- Mapped all 35 fields in Inspections dataset
- Mapped all 38 fields in Industrial Facilities dataset
- Mapped all 61 fields in Construction Facilities dataset
- Mapped all 27 fields in Industrial Monitoring dataset
- Mapped all 30 fields in Construction Monitoring dataset
- Identified data types, null patterns, and relationships

### 3. Integration Design ‚úÖ
- Designed unified Facility model linking eSMR + SMARTS
- Created 3-tier facility matching strategy
- Mapped all entity relationships (31 foreign keys)
- Designed 87 indexes for query optimization
- Addressed data quality issues (missing coords, invalid dates, etc.)

### 4. Deliverables ‚úÖ
- **Research report:** 10,000+ words comprehensive analysis
- **Prisma schema:** 24 tables, production-ready
- **Sample data:** 7 CSV files in `../data/samples/`
- **ER diagram:** Complete system visualization
- **Performance analysis:** Size estimates, index strategy, query patterns
- **Implementation plan:** 3-phase rollout over 8 weeks

---

## Critical Findings

### Finding #1: Facility Identification Challenge
**Problem:** eSMR uses `facilityPlaceId` (integer), SMARTS uses `WDID` (string) + `APP_ID` (string)

**Solution:** 
- Extended Facility model with both ID systems
- 3-tier matching: Direct parsing ‚Üí Fuzzy matching ‚Üí Manual mapping
- Accept that some facilities exist in only one system

**Impact:** ~70% auto-linkable, 30% manual verification needed

### Finding #2: Data Quality Issues
**Missing Coordinates:**
- Construction facilities: 60% missing lat/lon
- Industrial facilities: 15% missing lat/lon

**Missing Dates:**
- Violation discovery dates: 40% null
- Enforcement due dates: 58% null

**Financial Data:**
- Penalty amounts: >90% null in enforcement actions

**Mitigation:** Geocoding batch job, accept nulls where appropriate, display "Not reported" in UI

### Finding #3: Data Volume
**Total Estimated Records:** 8.5 million
**Total Database Size:** 6.5 GB initial, +700 MB/year growth

**Largest Tables:**
- SMARTSMonitoringSampleIndustrial: 5.5M records (2.5 GB)
- SMARTSMonitoringSampleConstruction: 2.8M records (1.2 GB)
- ESMRSample: 5M records (2 GB)

**Performance Strategy:**
- Batch inserts (1000 records at a time)
- Streaming CSV parser for large files
- Initial import runs locally (avoid Vercel timeout)
- Weekly incremental sync via cron

### Finding #4: Overlapping vs Complementary Data

**eSMR Monitoring ‚â† SMARTS Monitoring**
- eSMR: Wastewater effluent (NPDES permits)
- SMARTS: Stormwater runoff (General permits)
- **Conclusion:** Minimal overlap, import both

**eSMR Violations ‚â† SMARTS Violations**
- eSMR: Computed benchmark exceedances (proactive)
- SMARTS: Official regulatory violations (reactive)
- **Conclusion:** Different purposes, keep both

### Finding #5: Relationship Complexity

**Violations ‚Üí Enforcement:** Not 1:1, it's M:N
- One violation can trigger multiple enforcement actions (escalation)
- One enforcement action can address multiple violations
- SMARTS doesn't always link violations to enforcement explicitly

**Inspections ‚Üí Violations:** Also M:N
- One inspection can find multiple violations
- Violations can come from sources other than inspections
- Link via `VIOLATION_SOURCE` = "Inspection" and `VIOLATION_SOURCE_ID`

---

## Schema Design Highlights

### New Tables (14 total)
1. **SMARTSFacility** - Unified industrial + construction facilities
2. **SMARTSViolation** - Official violations from Water Board
3. **SMARTSEnforcementAction** - NNC, NOV, CAO, etc.
4. **SMARTSInspection** - Inspection records
5. **SMARTSMonitoringReport** - Storm event reports
6. **SMARTSMonitoringSample** - Individual parameter measurements
7. **SMARTSInspectionViolationLink** - M:N linking table
8. **SMARTSImportLog** - Track import jobs
9. **FacilityLink** - Manual facility linking table
10. **DataQualityIssue** - Track data quality problems
11. **GeocodeCache** - Cache geocoding API results

### Extended Tables (2)
1. **Facility** - Added `smartsWdid`, `smartsAppId` fields
2. **ESMRRegion** - Shared with SMARTS (universal regions)

### Indexes (87 total)
**High-priority composite indexes:**
- `[wdid, appId]` - Facility lookup (all SMARTS tables)
- `[wdid, sampleDate, parameter]` - Monitoring queries
- `[wdid, occurrenceDate]` - Violation queries
- `[facilityId, pollutantKey, detectedAt]` - eSMR violation queries

**Single-column indexes:**
- All dates (support range queries)
- All enums (support filtering)
- All foreign keys (support JOINs)

### Key Design Decisions

**1. Denormalization Preserved**
- Source SMARTS data is heavily denormalized (PLACE_* fields everywhere)
- Mirrored this in schema to simplify import
- Alternative (normalize into separate tables) rejected due to complexity

**2. Separate Monitoring Tables**
- Could combine ESMRSample + SMARTSMonitoringSample
- Kept separate due to different fields, update frequencies, query patterns
- Simpler to maintain independent sync jobs

**3. Single SMARTS Facility Table**
- Could split SMARTSFacilityIndustrial + SMARTSFacilityConstruction
- Combined into one table with nullable fields
- Reduces schema complexity, easier queries

**4. M:N Linking via Dedicated Tables**
- InspectionViolationLink table for explicit M:N relationship
- Cleaner than JSON arrays or denormalized fields
- Supports bidirectional queries

---

## Implementation Plan

### Phase 1: Violations + Enforcement (Weeks 1-3)
**Goal:** Add highest-value data first

**Tasks:**
- Create SMARTS schema (violations + enforcement tables)
- Download historical violations + enforcement CSVs
- Run local import script (one-time backfill)
- Create weekly cron sync job
- Add facility linking logic (Tier 1 + Tier 2)
- Create admin UI to view SMARTS violations
- Link violations to enforcement actions

**Deliverables:**
- 31K violations imported
- 29K enforcement actions imported
- >80% linked to facilities
- Weekly auto-sync operational

**Estimated Time:** 60-80 hours

### Phase 2: Inspections + Facilities (Weeks 4-5)
**Goal:** Add facility master data and inspection records

**Tasks:**
- Add SMARTSFacility table
- Import facility data (15K industrial + 78K construction)
- Enhance facility linking with name/location data
- Add SMARTSInspection table
- Import inspection records (45K)
- Link inspections to violations
- Add inspection history to facility profiles

**Deliverables:**
- 93K facilities imported
- 45K inspections imported
- Improved facility linking

**Estimated Time:** 30-40 hours

### Phase 3: Monitoring Data (Weeks 6-8)
**Goal:** Add monitoring sample data (large datasets)

**Tasks:**
- Add SMARTSMonitoringReport and SMARTSMonitoringSample tables
- Implement streaming CSV parser
- Import industrial monitoring (5.5M records)
- Import construction monitoring (2.8M records)
- Create monitoring data API endpoints
- Add monitoring charts to facility profiles
- Implement NAL exceedance detection

**Deliverables:**
- 8.3M monitoring samples imported
- Monitoring data queryable
- Charts showing trends
- Exceedance detection operational

**Estimated Time:** 60-80 hours

**Total Timeline:** 7-11 weeks (150-200 hours)

---

## Performance Projections

### Query Performance Estimates

| Query Type | Estimated Time | Notes |
|------------|---------------|-------|
| Facility violations (1 facility) | <100ms | Uses composite index |
| Regional violation summary (1 year) | <500ms | Uses date + status indexes |
| Monitoring exceedances (1 parameter) | 500ms-2s | May need materialized view |
| Facility compliance dashboard | <200ms | Multiple indexed queries |

### Database Growth

| Timeframe | Total Size | New Records |
|-----------|-----------|-------------|
| Initial | 6.5 GB | 8.5M records |
| Year 1 | 7.2 GB | +1.3M records |
| Year 3 | 8.6 GB | +3.9M records |
| Year 5 | 10.0 GB | +6.5M records |

**Archival Strategy (Future):**
- Move data older than 5 years to cold storage
- Keep recent 5 years in hot database
- Maintain summary statistics for historical trends

---

## Risks & Mitigations

### Risk #1: Import Performance
**Problem:** 8.5M records is too large for single import

**Mitigation:**
- Run initial import locally (not on Vercel)
- Use streaming CSV parser (line-by-line processing)
- Batch inserts (1000 records at a time)
- Weekly incremental updates (much smaller)

### Risk #2: Facility Linking Accuracy
**Problem:** Auto-matching may create false positives

**Mitigation:**
- 3-tier strategy (direct, fuzzy, manual)
- Confidence scoring (0.0-1.0)
- Manual verification for high-value facilities
- Allow users to report incorrect links

### Risk #3: Data Quality
**Problem:** Missing coordinates, invalid dates, null fields

**Mitigation:**
- Validation during import (log errors, don't fail)
- Geocoding batch job for missing coordinates
- Accept nulls, display "Not reported" in UI
- Track data quality issues in dedicated table

### Risk #4: Query Performance Degradation
**Problem:** 8.5M records may slow down queries

**Mitigation:**
- Comprehensive indexing (87 indexes)
- Monitor slow queries
- Consider partitioning if >10M records
- Materialize common aggregations

---

## Key Recommendations

### ‚úÖ DO THIS
1. **Start with Phase 1** (violations + enforcement) - highest value, manageable complexity
2. **Run initial import locally** - avoid Vercel timeout issues
3. **Implement streaming parser** - handle large files efficiently
4. **Use upsert logic** - prevent duplicates on weekly sync
5. **Monitor query performance** - add indexes as needed
6. **Manual verification** - top 100 facilities need manual linking

### ‚ùå DON'T DO THIS
1. **Don't try to import 8.5M records via Vercel cron** - will timeout
2. **Don't normalize PLACE_* fields** - adds complexity without benefit
3. **Don't combine eSMR + SMARTS monitoring** - different use cases
4. **Don't skip validation** - data quality issues will compound
5. **Don't partition prematurely** - wait until performance issues arise

### üîÆ FUTURE ENHANCEMENTS
1. **Geocoding API integration** - batch fill missing coordinates
2. **Materialized views** - pre-compute common aggregations
3. **Data archival** - move old data to cold storage
4. **User-reported corrections** - crowdsource facility linking
5. **Advanced analytics** - enforcement trends, repeat offenders

---

## Verification Checklist

- [x] Sample data downloaded for all 7 SMARTS datasets
- [x] Current eSMR structure documented
- [x] Complete field inventory for all 8 datasets
- [x] Data quality metrics calculated
- [x] All entity relationships mapped
- [x] Facility linking strategy defined
- [x] Complete Prisma schema created
- [x] All foreign keys and indexes specified
- [x] ER diagram created
- [x] Performance analysis with size estimates
- [x] Implementation roadmap with migration steps
- [x] Research report completed
- [x] Proposed schema saved

---

## Next Actions

### For Product Team
1. Review research report and proposed schema
2. Prioritize phases based on business value
3. Allocate development resources (150-200 hours)
4. Plan user testing for new features

### For Development Team
1. Review Prisma schema for technical concerns
2. Set up local environment for data import
3. Download historical SMARTS datasets
4. Begin Phase 1 implementation (violations + enforcement)

### For Operations
1. Provision additional database storage (6.5 GB initial)
2. Set up monitoring for import jobs
3. Plan backup strategy for large database
4. Configure alerting for failed sync jobs

---

**Research Status:** ‚úÖ Complete
**Next Milestone:** Schema implementation and Phase 1 import
**Questions?** Review full report at `./data-schema-integration-analysis.md`

---

*Generated by: Claude Sonnet 4.5*
*Date: December 6, 2025*
*Research Duration: ~6 hours*
